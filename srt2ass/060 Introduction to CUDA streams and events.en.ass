[Script Info]; Script generated by FFmpeg/Lavc59.18.100ScriptType: v4.00+PlayResX: 384PlayResY: 288ScaledBorderAndShadow: yes[V4+ Styles]Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, EncodingStyle: Default,Helvetica,10,&Hffffff,&Hffffff,&H0,&H0,0,0,0,0,100,100,0,0,1,1,0,2,10,10,10,0[Events]Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, TextDialogue:0,00:00:03.55,00:00:08.63,Default,,0,0,0,,From this video onwards, we are going to start our discussion on CUDA streams.Dialogue:0,00:00:08.98,00:00:15.43,Default,,0,0,0,,So far, our sole purpose was to improve performance of a particular kernel based on CUDA programmingDialogue:0,00:00:15.43,00:00:18.57,Default,,0,0,0,,model, execution model and memory model.Dialogue:0,00:00:19.70,00:00:26.39,Default,,0,0,0,,But in this section we are going to discuss ways of improving overall program execution by dividingDialogue:0,00:00:26.39,00:00:32.99,Default,,0,0,0,,workload among multiple kernels and executing those kernels concurrently on a device.Dialogue:0,00:00:33.11,00:00:40.68,Default,,0,0,0,,So far in this course, we followed strict model for launching a kernel. We first allocate and transfer dataDialogue:0,00:00:40.68,00:00:44.69,Default,,0,0,0,,to the device from the host and then we launch the kernel,Dialogue:0,00:00:44.79,00:00:50.90,Default,,0,0,0,,and after that we wait until the kernel execution finish using cudaDeviceSynchronize function,Dialogue:0,00:00:51.09,00:00:54.93,Default,,0,0,0,,and then we transfer the results back to the host.Dialogue:0,00:00:54.97,00:01:01.78,Default,,0,0,0,,This is the model we followed so far. Here we achieved the performance by executing one kernel parallelDialogue:0,00:01:01.99,00:01:04.33,Default,,0,0,0,,on multiple data at the same time.Dialogue:0,00:01:04.68,00:01:11.17,Default,,0,0,0,,Parallelism we saw here normally refers to as the kernel level parallelism. Parallelism we are goingDialogue:0,00:01:11.17,00:01:16.95,Default,,0,0,0,,to discuss in this section is called grid level parallelism. In grid level parallelismDialogue:0,00:01:16.99,00:01:23.83,Default,,0,0,0,,concurrency is achieved by launching multiple kernels to same device simultaneously and overlappingDialogue:0,00:01:23.83,00:01:26.43,Default,,0,0,0,,memory transfers with kernel execution.Dialogue:0,00:01:26.46,00:01:31.96,Default,,0,0,0,,In our previous CUDA programmes, we transferred memory at the beginning of the program.Dialogue:0,00:01:32.53,00:01:37.31,Default,,0,0,0,,But as you may already  notice , device has limited amount of resources.Dialogue:0,00:01:37.45,00:01:41.33,Default,,0,0,0,,So it cannot operate on the all the data we pass simultaneously.Dialogue:0,00:01:41.62,00:01:48.51,Default,,0,0,0,,So what if we partition our data and transfer only partition enough to execute one kernel optimallyDialogue:0,00:01:48.92,00:01:54.38,Default,,0,0,0,,and while that kernel is executing on that partition we transfer another partition of dataDialogue:0,00:01:54.43,00:01:59.80,Default,,0,0,0,,and so on. In this way we can overlap kernel execution with memory transferring.Dialogue:0,00:02:00.31,00:02:03.22,Default,,0,0,0,,So overall execution time is going to reduce,Dialogue:0,00:02:03.28,00:02:09.13,Default,,0,0,0,,because of this operation overlapping as shown in this diagram. To achieve this type of overlappingDialogue:0,00:02:09.13,00:02:16.36,Default,,0,0,0,,between operations we need a way to launch multiple kernels on the same device and we need a way to transferDialogue:0,00:02:16.36,00:02:18.16,Default,,0,0,0,,memory asynchronously.Dialogue:0,00:02:18.67,00:02:23.30,Default,,0,0,0,,This is where CUDA streams and asynchronous functions come to our rescue.Dialogue:0,00:02:23.68,00:02:32.15,Default,,0,0,0,,So let's first look  at what is a CUDA stream now. A stream is a sequence of command that execute in order.Dialogue:0,00:02:32.68,00:02:37.08,Default,,0,0,0,,So within a single stream operations follow strict ordering.Dialogue:0,00:02:37.18,00:02:44.29,Default,,0,0,0,,For example, if we put all the partition data chunks data kernels to one stream, then above mentionDialogue:0,00:02:44.29,00:02:50.26,Default,,0,0,0,,operation overlapping would not be possible. But different streams on the other hand may execute their commandDialogue:0,00:02:50.50,00:02:54.22,Default,,0,0,0,,without any kind of ordering with respect to other streams.Dialogue:0,00:02:54.28,00:03:00.91,Default,,0,0,0,,So the way to achieve above mention overlapping is to put memory operations and kernel launches for one dataDialogue:0,00:03:00.91,00:03:07.51,Default,,0,0,0,,partition in to a unique stream. We will discuss more in this in upcoming video.Dialogue:0,00:03:07.54,00:03:08.08,Default,,0,0,0,,Now,Dialogue:0,00:03:08.17,00:03:11.32,Default,,0,0,0,,let's look at CUDA asynchronous operations.Dialogue:0,00:03:11.68,00:03:18.00,Default,,0,0,0,,When we talk about synchronous or asynchronous behaviors of operation in CUDA we have to consider bothDialogue:0,00:03:18.00,00:03:20.38,Default,,0,0,0,,the host and device perspective.Dialogue:0,00:03:20.68,00:03:26.80,Default,,0,0,0,,Let's start the discussion from the host perspective. Function with synchronous behavior relative to the hostDialogue:0,00:03:26.80,00:03:34.39,Default,,0,0,0,,block the host thread until they complete. On the other hand functions with asynchronous behaviorsDialogue:0,00:03:34.72,00:03:41.23,Default,,0,0,0,,return control to the host immediately after being called. For example, in our previous implementationsDialogue:0,00:03:41.35,00:03:47.34,Default,,0,0,0,,memory copy function calls, memory set function calls and cudaDeviceSynchronize function calls were synchronousDialogue:0,00:03:47.35,00:03:48.45,Default,,0,0,0,,function calls.Dialogue:0,00:03:48.64,00:03:54.18,Default,,0,0,0,,They block the host code execution. But kernel launches are asynchronous operationsDialogue:0,00:03:54.24,00:04:01.78,Default,,0,0,0,,so control was immediately return to the host after kernel launch instruction executed. Host does not has to wait untilDialogue:0,00:04:01.96,00:04:09.19,Default,,0,0,0,,kernel execution is finished in the device. Like I mention, synchronous and asynchronous behaviours depend onDialogue:0,00:04:09.21,00:04:14.13,Default,,0,0,0,,whether you looking at function call from host point of view or device point of view.Dialogue:0,00:04:15.26,00:04:22.14,Default,,0,0,0,,Consider the kernel launch statement in this slide. Here I have launch 3 kernels in different streamsDialogue:0,00:04:22.14,00:04:23.26,Default,,0,0,0,,from the host.Dialogue:0,00:04:23.38,00:04:30.93,Default,,0,0,0,,Notice here, we launch second kernel using default stream or null stream. Now from the host point of viewDialogue:0,00:04:31.05,00:04:33.58,Default,,0,0,0,,all the kernel launches are asynchronous.Dialogue:0,00:04:33.78,00:04:40.38,Default,,0,0,0,,So host will not wait until any of these kernels are finish unless explicitly wait using synchronize function calls.Dialogue:0,00:04:40.38,00:04:41.17,Default,,0,0,0,,cDialogue:0,00:04:41.36,00:04:49.05,Default,,0,0,0,,But from the device point of view, these kernel launches may or may not be executed in device at the same time depending on theDialogue:0,00:04:49.05,00:04:51.94,Default,,0,0,0,,relationship stream 1 and stream 3 haveDialogue:0,00:04:51.96,00:04:53.83,Default,,0,0,0,,with the default stream.Dialogue:0,00:04:54.18,00:04:59.73,Default,,0,0,0,,So in this case even though these kernel launches are asynchronous relative to the host, but in the deviceDialogue:0,00:04:59.73,00:05:05.92,Default,,0,0,0,,these kernel launches may or may not be asynchronous. In a upcoming video,Dialogue:0,00:05:06.00,00:05:09.35,Default,,0,0,0,,you will see reasoning behind this kind of behaviour as well.Dialogue:0,00:05:10.22,00:05:18.87,Default,,0,0,0,,Ok, let's now see what the null stream is. The null stream is the default stream that kernel launches and data transfersDialogue:0,00:05:18.90,00:05:19.61,Default,,0,0,0,,useDialogue:0,00:05:19.68,00:05:26.93,Default,,0,0,0,,if you do not explicitly specify the different stream. In all the examples we seen so far, we did perform CUDADialogue:0,00:05:26.93,00:05:29.31,Default,,0,0,0,,operations in the NULL stream.Dialogue:0,00:05:30.47,00:05:36.75,Default,,0,0,0,,Apart from all the things we done so far NULL stream has synchronization relationship with other asynchronous streams,Dialogue:0,00:05:36.76,00:05:43.68,Default,,0,0,0,,so null stream is commonly use as synchronization mechanism multiple streamsDialogue:0,00:05:43.75,00:05:50.62,Default,,0,0,0,,as you will see in the upcoming videos. Ok to wind of this video let me list some of the operationsDialogue:0,00:05:50.65,00:05:57.10,Default,,0,0,0,,which can be performed independently hence which can be overlapped or can perform concurrently usingDialogue:0,00:05:57.22,00:06:05.49,Default,,0,0,0,,different stream. Computations on the host, computations on the device, memory transfers from host to device,Dialogue:0,00:06:05.49,00:06:13.84,Default,,0,0,0,,memory transfers from the device to host, memory transfers within memory of a given device andDialogue:0,00:06:13.99,00:06:16.20,Default,,0,0,0,,memory transfer among devices.Dialogue:0,00:06:16.30,00:06:22.80,Default,,0,0,0,,So in up coming videos you will see how to perform these operations concurrently using non-null streams.