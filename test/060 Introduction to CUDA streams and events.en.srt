1
00:00:03,550 --> 00:00:08,630
From this video onwards, we are going to start our discussion on CUDA streams.

2
00:00:08,980 --> 00:00:15,430
So far, our sole purpose was to improve performance of a particular kernel based on CUDA programming

3
00:00:15,430 --> 00:00:18,570
model, execution model and memory model.

4
00:00:19,700 --> 00:00:26,390
But in this section we are going to discuss ways of improving overall program execution by dividing

5
00:00:26,390 --> 00:00:32,990
workload among multiple kernels and executing those kernels concurrently on a device.

6
00:00:33,110 --> 00:00:40,680
So far in this course, we followed strict model for launching a kernel. We first allocate and transfer data

7
00:00:40,680 --> 00:00:44,690
to the device from the host and then we launch the kernel,

8
00:00:44,790 --> 00:00:50,900
and after that we wait until the kernel execution finish using cudaDeviceSynchronize function,

9
00:00:51,090 --> 00:00:54,930
and then we transfer the results back to the host.

10
00:00:54,970 --> 00:01:01,780
This is the model we followed so far. Here we achieved the performance by executing one kernel parallel

11
00:01:01,990 --> 00:01:04,330
on multiple data at the same time.

12
00:01:04,680 --> 00:01:11,170
Parallelism we saw here normally refers to as the kernel level parallelism. Parallelism we are going

13
00:01:11,170 --> 00:01:16,950
to discuss in this section is called grid level parallelism. In grid level parallelism

14
00:01:16,990 --> 00:01:23,830
concurrency is achieved by launching multiple kernels to same device simultaneously and overlapping

15
00:01:23,830 --> 00:01:26,430
memory transfers with kernel execution.

16
00:01:26,460 --> 00:01:31,960
In our previous CUDA programmes, we transferred memory at the beginning of the program.

17
00:01:32,530 --> 00:01:37,310
But as you may already  notice , device has limited amount of resources.

18
00:01:37,450 --> 00:01:41,330
So it cannot operate on the all the data we pass simultaneously.

19
00:01:41,620 --> 00:01:48,510
So what if we partition our data and transfer only partition enough to execute one kernel optimally

20
00:01:48,920 --> 00:01:54,380
and while that kernel is executing on that partition we transfer another partition of data

21
00:01:54,430 --> 00:01:59,800
and so on. In this way we can overlap kernel execution with memory transferring.

22
00:02:00,310 --> 00:02:03,220
So overall execution time is going to reduce,

23
00:02:03,280 --> 00:02:09,130
because of this operation overlapping as shown in this diagram. To achieve this type of overlapping

24
00:02:09,130 --> 00:02:16,360
between operations we need a way to launch multiple kernels on the same device and we need a way to transfer

25
00:02:16,360 --> 00:02:18,160
memory asynchronously.

26
00:02:18,670 --> 00:02:23,300
This is where CUDA streams and asynchronous functions come to our rescue.

27
00:02:23,680 --> 00:02:32,150
So let's first look  at what is a CUDA stream now. A stream is a sequence of command that execute in order.

28
00:02:32,680 --> 00:02:37,080
So within a single stream operations follow strict ordering.

29
00:02:37,180 --> 00:02:44,290
For example, if we put all the partition data chunks data kernels to one stream, then above mention

30
00:02:44,290 --> 00:02:50,260
operation overlapping would not be possible. But different streams on the other hand may execute their command

31
00:02:50,500 --> 00:02:54,220
without any kind of ordering with respect to other streams.

32
00:02:54,280 --> 00:03:00,910
So the way to achieve above mention overlapping is to put memory operations and kernel launches for one data

33
00:03:00,910 --> 00:03:07,510
partition in to a unique stream. We will discuss more in this in upcoming video.

34
00:03:07,540 --> 00:03:08,080
Now,

35
00:03:08,170 --> 00:03:11,320
let's look at CUDA asynchronous operations.

36
00:03:11,680 --> 00:03:18,000
When we talk about synchronous or asynchronous behaviors of operation in CUDA we have to consider both

37
00:03:18,000 --> 00:03:20,380
the host and device perspective.

38
00:03:20,680 --> 00:03:26,800
Let's start the discussion from the host perspective. Function with synchronous behavior relative to the host

39
00:03:26,800 --> 00:03:34,390
block the host thread until they complete. On the other hand functions with asynchronous behaviors

40
00:03:34,720 --> 00:03:41,230
return control to the host immediately after being called. For example, in our previous implementations

41
00:03:41,350 --> 00:03:47,340
memory copy function calls, memory set function calls and cudaDeviceSynchronize function calls were synchronous

42
00:03:47,350 --> 00:03:48,450
function calls.

43
00:03:48,640 --> 00:03:54,180
They block the host code execution. But kernel launches are asynchronous operations

44
00:03:54,240 --> 00:04:01,780
so control was immediately return to the host after kernel launch instruction executed. Host does not has to wait until

45
00:04:01,960 --> 00:04:09,190
kernel execution is finished in the device. Like I mention, synchronous and asynchronous behaviours depend on

46
00:04:09,210 --> 00:04:14,130
whether you looking at function call from host point of view or device point of view.

47
00:04:15,260 --> 00:04:22,140
Consider the kernel launch statement in this slide. Here I have launch 3 kernels in different streams

48
00:04:22,140 --> 00:04:23,260
from the host.

49
00:04:23,380 --> 00:04:30,930
Notice here, we launch second kernel using default stream or null stream. Now from the host point of view

50
00:04:31,050 --> 00:04:33,580
all the kernel launches are asynchronous.

51
00:04:33,780 --> 00:04:40,380
So host will not wait until any of these kernels are finish unless explicitly wait using synchronize function calls.

52
00:04:40,380 --> 00:04:41,170
c

53
00:04:41,360 --> 00:04:49,050
But from the device point of view, these kernel launches may or may not be executed in device at the same time depending on the

54
00:04:49,050 --> 00:04:51,940
relationship stream 1 and stream 3 have

55
00:04:51,960 --> 00:04:53,830
with the default stream.

56
00:04:54,180 --> 00:04:59,730
So in this case even though these kernel launches are asynchronous relative to the host, but in the device

57
00:04:59,730 --> 00:05:05,920
these kernel launches may or may not be asynchronous. In a upcoming video,

58
00:05:06,000 --> 00:05:09,350
you will see reasoning behind this kind of behaviour as well.

59
00:05:10,220 --> 00:05:18,870
Ok, let's now see what the null stream is. The null stream is the default stream that kernel launches and data transfers

60
00:05:18,900 --> 00:05:19,610
use

61
00:05:19,680 --> 00:05:26,930
if you do not explicitly specify the different stream. In all the examples we seen so far, we did perform CUDA

62
00:05:26,930 --> 00:05:29,310
operations in the NULL stream.

63
00:05:30,470 --> 00:05:36,750
Apart from all the things we done so far NULL stream has synchronization relationship with other asynchronous streams,

64
00:05:36,760 --> 00:05:43,680
so null stream is commonly use as synchronization mechanism multiple streams

65
00:05:43,750 --> 00:05:50,620
as you will see in the upcoming videos. Ok to wind of this video let me list some of the operations

66
00:05:50,650 --> 00:05:57,100
which can be performed independently hence which can be overlapped or can perform concurrently using

67
00:05:57,220 --> 00:06:05,490
different stream. Computations on the host, computations on the device, memory transfers from host to device,

68
00:06:05,490 --> 00:06:13,840
memory transfers from the device to host, memory transfers within memory of a given device and

69
00:06:13,990 --> 00:06:16,200
memory transfer among devices.

70
00:06:16,300 --> 00:06:22,800
So in up coming videos you will see how to perform these operations concurrently using non-null streams.
